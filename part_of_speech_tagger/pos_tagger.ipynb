{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping STTS nach Universal Tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping STTS nach Universal Tagset\n",
    "# Das Mapping wurde grundsätzlich aus folgender Quelle übernommen:\n",
    "# https://raw.githubusercontent.com/slavpetrov/universal-pos-tags/master/de-tiger.map\n",
    "# und mit dieser Quelle überprüft.\n",
    "# https://pdfs.semanticscholar.org/ed2c/c779c7eb0004bd6dd50538a2cafca092c94f.pdf\n",
    "# \n",
    "# Aufgrund von TIGER_scheme-syntax.pfd Seite 122ff. wurden folgende Anpassungen vorgenommen: \n",
    "# NNE gelöscht, da nicht Teil des Tagsets\n",
    "# \n",
    "# Folgende drei Tags wurden hinzugefügt, obwohl diese nicht im Trainingset vorkommen,\n",
    "# diese könnten jedoch im Testset des Dozenten auftauchen.\n",
    "# \n",
    "# PAV PRON (Gleichbedeutend wie PROAV) \n",
    "# PIDAT PRON\n",
    "# SGML  X\n",
    "# SPELL X\n",
    "\n",
    "stts_to_universal = {\n",
    "\"$(\":\".\",\n",
    "\"$,\":\".\",\n",
    "\"$.\":\".\",\n",
    "\"ADJA\":\"ADJ\",\n",
    "\"ADJD\":\"ADJ\",\n",
    "\"ADV\":\"ADV\",\n",
    "\"APPO\":\"ADP\",\n",
    "\"APPR\":\"ADP\",\n",
    "\"APPRART\":\"ADP\",\n",
    "\"APZR\":\"ADP\",\n",
    "\"ART\":\"DET\",\n",
    "\"CARD\":\"NUM\",\n",
    "\"FM\":\"X\",\n",
    "\"ITJ\":\"X\",\n",
    "\"KOKOM\":\"CONJ\",\n",
    "\"KON\":\"CONJ\",\n",
    "\"KOUI\":\"CONJ\",\n",
    "\"KOUS\":\"CONJ\",\n",
    "\"NE\":\"NOUN\",\n",
    "\"NN\":\"NOUN\",\n",
    "\"PDAT\":\"PRON\",\n",
    "\"PDS\":\"PRON\",\n",
    "\"PIAT\":\"PRON\",\n",
    "\"PIS\":\"PRON\",\n",
    "\"PPER\":\"PRON\",\n",
    "\"PPOSAT\":\"PRON\",\n",
    "\"PPOSS\":\"PRON\",\n",
    "\"PRELAT\":\"PRON\",\n",
    "\"PRELS\":\"PRON\",\n",
    "\"PRF\":\"PRON\",\n",
    "\"PAV\":\"PRON\",\n",
    "\"PROAV\":\"PRON\",\n",
    "\"PTKA\":\"PRT\",\n",
    "\"PTKANT\":\"PRT\",\n",
    "\"PTKNEG\":\"PRT\",\n",
    "\"PTKVZ\":\"PRT\",\n",
    "\"PTKZU\":\"PRT\",\n",
    "\"PWAT\":\"PRON\",\n",
    "\"PWAV\":\"PRON\",\n",
    "\"PWS\":\"PRON\",\n",
    "\"TRUNC\":\"X\",\n",
    "\"VAFIN\":\"VERB\",\n",
    "\"VAIMP\":\"VERB\",\n",
    "\"VAINF\":\"VERB\",\n",
    "\"VAPP\":\"VERB\",\n",
    "\"VMFIN\":\"VERB\",\n",
    "\"VMINF\":\"VERB\",\n",
    "\"VMPP\":\"VERB\",\n",
    "\"VVFIN\":\"VERB\",\n",
    "\"VVIMP\":\"VERB\",\n",
    "\"VVINF\":\"VERB\",\n",
    "\"VVIZU\":\"VERB\",\n",
    "\"VVPP\":\"VERB\",\n",
    "\"XY\":\"X\",\n",
    "\"PIDAT\":\"PRON\",\n",
    "\"SGML\":\"X\",\n",
    "\"SPELL\":\"X\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_sentences(raw_lines):\n",
    "    tagged_sentences = []\n",
    "    for line in raw_lines:\n",
    "        if line:\n",
    "            tuples = [tuple(word_and_tag.strip().rsplit('/', 1)) \n",
    "                      for word_and_tag \n",
    "                      in line.strip()[:-1].split(\" ; \")]\n",
    "            tagged_sentences.append(tuples)\n",
    "    return tagged_sentences\n",
    "\n",
    "def create_index(sentences, special_values=[]):\n",
    "    bag_of_words = [word for sentence in sentences for word in sentence]\n",
    "    \n",
    "    fdist_words = nltk.FreqDist(bag_of_words)\n",
    "\n",
    "    index = {value : i + len(special_values) for i, value in enumerate(fdist_words.keys())} \n",
    "    \n",
    "    for i, value in enumerate(special_values): \n",
    "        index[value]  = i\n",
    "\n",
    "    reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "\n",
    "    return index, reverse_index\n",
    "\n",
    "def translate(text, dictionnary, backup_value): \n",
    "    return np.array([dictionnary.get(value, backup_value) for value in text])\n",
    "        \n",
    "def split_tuples(arrays_of_tuples):\n",
    "    arr_left, arr_right  = [], []\n",
    "\n",
    "    for arr in arrays_of_tuples:\n",
    "        left, right = zip(*arr)\n",
    "\n",
    "        arr_left.append(np.asarray(left))\n",
    "        arr_right.append(np.asarray(right))\n",
    "    return arr_left, arr_right\n",
    "\n",
    "def to_categorical_reverse(categorical_sents):\n",
    "    # Keras includes method to_categorical but not the reverse operation\n",
    "    # Method adapted from\n",
    "    # https://stackoverflow.com/questions/47380663/numpy-reverse-keras-to-categorical\n",
    "    categorical_sents_reversed = []\n",
    "    for sent in categorical_sents:\n",
    "        categorical_sents_reversed.append(np.array([np.argmax(y, axis=None, out=None) for y in sent]))\n",
    "\n",
    "    return np.array(categorical_sents_reversed)\n",
    "\n",
    "\n",
    "def split_too_long_sentences(sentences, max_length):\n",
    "    sentences_splitted = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        while len(sent) > max_length:\n",
    "            print(\"Too long sentence\", len(sent), \"at index\", i, \n",
    "                  \"splitting into\", len(sent[:max_length]), \"and\", len(sent[max_length:]))\n",
    "            sentences_splitted.append(sent[:max_length])\n",
    "            sent = sent[max_length:]\n",
    "\n",
    "        sentences_splitted.append(sent)\n",
    "        \n",
    "    return sentences_splitted\n",
    "\n",
    "def remove_padding(sequences_padded, sequences_target_length):\n",
    "    sequences_without_padding = []    \n",
    "    \n",
    "    for i, sent in enumerate(sequences_target_length):\n",
    "        sequences_without_padding.append(sequences_padded[i][:len(sent)])\n",
    "    \n",
    "    return sequences_without_padding\n",
    "\n",
    "def plot_history(h):\n",
    "    # copied from TensorFlow_Intro.ipynb\n",
    "    history_dict = h.history\n",
    "    history_dict.keys()\n",
    "    \n",
    "    acc = h.history['acc']\n",
    "    val_acc = h.history['val_acc']\n",
    "    loss = h.history['loss']\n",
    "    val_loss = h.history['val_loss']\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs,     loss, 'bo', label='Training loss')      # \"bo\" is for \"blue dot\"\n",
    "    plt.plot(epochs, val_loss, 'b' , label='Validation loss')    # b is for \"solid blue line\"\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.0,0.2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    plt.clf()   # clear figure\n",
    "    acc_values = history_dict['acc']\n",
    "    val_acc_values = history_dict['val_acc']\n",
    "\n",
    "    plt.plot(epochs,     acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b' , label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.92,1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files öffnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sent Length   130\n",
      "Too long sentence 143 at index 221 splitting into 130 and 13\n",
      "Length Test orig  472 and after split 473\n",
      "Raw Line:         Konzernchefs/NN ; lehnen/VVFIN ; den/ART ; Milliardaer/NN ; als/APPR ; US-Praesi\n",
      "Tagged sentence   [('Konzernchefs', 'NN'), ('lehnen', 'VVFIN'), ('den', 'ART'), ('Milliardaer', 'NN'), ('als', 'APPR')]\n",
      "Words             ['Konzernchefs' 'lehnen' 'den' 'Milliardaer' 'als']\n",
      "Tags STTS         ['NN' 'VVFIN' 'ART' 'NN' 'APPR']\n",
      "Tags Universal    ['NOUN' 'VERB' 'DET' 'NOUN' 'ADP']\n",
      "\n",
      "Raw Line:         Qualifikation/NN ; und/KON ; Ausbildung/NN ; von/APPR ; Mitarbeitern/NN ; privat\n",
      "Tagged sentence   [('Qualifikation', 'NN'), ('und', 'KON'), ('Ausbildung', 'NN'), ('von', 'APPR'), ('Mitarbeitern', 'NN')]\n",
      "Words             ['Qualifikation' 'und' 'Ausbildung' 'von' 'Mitarbeitern']\n",
      "Tags STTS         ['NN' 'KON' 'NN' 'APPR' 'NN']\n",
      "Tags Universal    ['NOUN' 'CONJ' 'NOUN' 'ADP' 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "train_text_file = open(\"POS_German_train.txt\", \"r\")\n",
    "test_text_file  = open(\"POS_German_minitest.txt\", \"r\")\n",
    "\n",
    "train_lines = train_text_file.read().split('\\n')\n",
    "test_lines  = test_text_file.read().split('\\n')\n",
    "\n",
    "# create sents with (word, tag) tuples\n",
    "train_tagged_sents      = get_tagged_sentences(train_lines)\n",
    "test_tagged_sents       = get_tagged_sentences(test_lines)\n",
    "\n",
    "# max sentence length based on longest train sentences (test sentences are considered unkonwn at first)\n",
    "train_max_sent_length   = max(len(sentence) for sentence in train_tagged_sents)\n",
    "print(\"Max Sent Length  \", train_max_sent_length)\n",
    "# split too long test sentences, usually it would be done in a separate data cleansing step\n",
    "\n",
    "number_of_test_sents_orig = len(test_tagged_sents)\n",
    "test_tagged_sents       = split_too_long_sentences(test_tagged_sents, train_max_sent_length)\n",
    "print(\"Length Test orig \", number_of_test_sents_orig, \"and after split\", len(test_tagged_sents))\n",
    "\n",
    "# split (word, tag) into separate sents arrays\n",
    "train_sents_words, train_sents_tags_stts = split_tuples(train_tagged_sents)\n",
    "test_sents_words,  test_sents_tags_stts  = split_tuples(test_tagged_sents)\n",
    "\n",
    "# map stts to universal tagset\n",
    "train_sents_tags = [translate(t, stts_to_universal, \"tag not found\") for t in train_sents_tags_stts]\n",
    "test_sents_tags  = [translate(t, stts_to_universal, \"tag not found\") for t in test_sents_tags_stts]\n",
    "\n",
    "\n",
    "print(\"Raw Line:        \", train_lines[1][:80])\n",
    "print(\"Tagged sentence  \", train_tagged_sents[1][:5])\n",
    "print(\"Words            \", train_sents_words[1][:5])\n",
    "print(\"Tags STTS        \", train_sents_tags_stts[1][:5])\n",
    "print(\"Tags Universal   \", train_sents_tags[1][:5])\n",
    "print()\n",
    "print(\"Raw Line:        \", test_lines[1][:80])\n",
    "print(\"Tagged sentence  \", test_tagged_sents[1][:5])\n",
    "print(\"Words            \", test_sents_words[1][:5])\n",
    "print(\"Tags STTS        \", test_sents_tags_stts[1][:5])\n",
    "print(\"Tags Universal   \", test_sents_tags[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Mapping STTS nach Universal Tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$( .\n",
      "$, .\n",
      "$. .\n",
      "ADJA ADJ\n",
      "ADJD ADJ\n",
      "ADV ADV\n",
      "APPO ADP\n",
      "APPR ADP\n",
      "APPRART ADP\n",
      "APZR ADP\n",
      "ART DET\n",
      "CARD NUM\n",
      "FM X\n",
      "ITJ X\n",
      "KOKOM CONJ\n",
      "KON CONJ\n",
      "KOUI CONJ\n",
      "KOUS CONJ\n",
      "NE NOUN\n",
      "NN NOUN\n",
      "PAV PRON\n",
      "PDAT PRON\n",
      "PDS PRON\n",
      "PIAT PRON\n",
      "PIS PRON\n",
      "PPER PRON\n",
      "PPOSAT PRON\n",
      "PPOSS PRON\n",
      "PRELAT PRON\n",
      "PRELS PRON\n",
      "PRF PRON\n",
      "PTKA PRT\n",
      "PTKANT PRT\n",
      "PTKNEG PRT\n",
      "PTKVZ PRT\n",
      "PTKZU PRT\n",
      "PWAT PRON\n",
      "PWAV PRON\n",
      "PWS PRON\n",
      "TRUNC X\n",
      "VAFIN VERB\n",
      "VAIMP VERB\n",
      "VAINF VERB\n",
      "VAPP VERB\n",
      "VMFIN VERB\n",
      "VMINF VERB\n",
      "VMPP VERB\n",
      "VVFIN VERB\n",
      "VVIMP VERB\n",
      "VVINF VERB\n",
      "VVIZU VERB\n",
      "VVPP VERB\n",
      "XY X\n"
     ]
    }
   ],
   "source": [
    "def test_stts_to_universal_mapping(sentences_tags_stts):\n",
    "    bag_of_tags = [tag for sentence in sentences_tags_stts for tag in sentence]\n",
    "    stts_tags = sorted(nltk.FreqDist(bag_of_tags).keys())    \n",
    "    universal_tags = translate(stts_tags, stts_to_universal, \"error, tag not found\")\n",
    "\n",
    "    for (stts, universal) in zip(stts_tags, universal_tags):\n",
    "        print(stts, universal)\n",
    "\n",
    "test_stts_to_universal_mapping(train_sents_tags_stts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusätzliches Test/Dev Set erstellen\n",
    "Es wird mit vier Sets gearbeitet\n",
    "\n",
    "- Train\n",
    "- Test\n",
    "\n",
    "für möglichst hohe accuracy im POS_German_minitest.txt und \n",
    "\n",
    "- Train Partial (90% des Train sets)\n",
    "- Dev (10% des Train sets)\n",
    "\n",
    "während der Entwicklung zur Erabeitung möglichst guter Hyperparameter.\n",
    "\n",
    "\n",
    "### Mapping Word>Index, Tag>Index und umgekehrt anlegen\n",
    "\n",
    "Auch hier werden Mappings mit gesamtem Train Set erstellt und auch mit 90% der Trainings Daten zum Testen des Dev Sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train         / Test sents:   40000 473\n",
      "Length Train Partial / Dev  sents:   36000 4000\n",
      "Length          Word / Tag Index:    76658 13\n",
      "Length      Dev Word / Tag Index:    71886 13\n",
      "Word to index:      [('Schiesser', 76656), ('Waeschefirma', 76657), ('<PAD>', 0), ('<UNK>', 1)]\n",
      "Index to word:      [(76656, 'Schiesser'), (76657, 'Waeschefirma'), (0, '<PAD>'), (1, '<UNK>')]\n",
      "Word to index Dev:  [('KMK-Praesident', 71884), ('Hochschulausgaben', 71885), ('<PAD>', 0), ('<UNK>', 1)]\n",
      "Index to word Dev:  [(71884, 'KMK-Praesident'), (71885, 'Hochschulausgaben'), (0, '<PAD>'), (1, '<UNK>')]\n",
      "('.', 1) (1, '.')\n",
      "('NOUN', 2) (2, 'NOUN')\n",
      "('VERB', 3) (3, 'VERB')\n",
      "('ADV', 4) (4, 'ADV')\n",
      "('DET', 5) (5, 'DET')\n",
      "('ADJ', 6) (6, 'ADJ')\n",
      "('ADP', 7) (7, 'ADP')\n",
      "('PRT', 8) (8, 'PRT')\n",
      "('PRON', 9) (9, 'PRON')\n",
      "('X', 10) (10, 'X')\n",
      "('CONJ', 11) (11, 'CONJ')\n",
      "('NUM', 12) (12, 'NUM')\n",
      "('<PAD>', 0) (0, '<PAD>')\n"
     ]
    }
   ],
   "source": [
    "# split and shuffle training set again into test and devset\n",
    "(train_sents_words_partial, \n",
    " dev_sents_words,\n",
    " train_sents_tags_partial,\n",
    " dev_sents_tags) = train_test_split(\n",
    "                        train_sents_words,\n",
    "                        train_sents_tags,\n",
    "                        test_size=0.1)\n",
    "\n",
    "# create index with 100% of train_sentences\n",
    "word_to_index, index_to_word = create_index(train_sents_words, [\"<PAD>\",\"<UNK>\"] )\n",
    "tag_to_index,  index_to_tag  = create_index(train_sents_tags, [\"<PAD>\"])\n",
    "\n",
    "# create index only with 90% of data (train_sents_words_partial)\n",
    "word_to_index_dev, index_to_word_dev = create_index(train_sents_words_partial, [\"<PAD>\",\"<UNK>\"] )\n",
    "tag_to_index_dev,  index_to_tag_dev  = create_index(train_sents_tags_partial, [\"<PAD>\"])\n",
    "\n",
    "\n",
    "print(\"Length Train         / Test sents:  \", len(train_sents_words), len(test_sents_words))\n",
    "print(\"Length Train Partial / Dev  sents:  \", len(train_sents_words_partial), len(dev_sents_words))\n",
    "\n",
    "print(\"Length          Word / Tag Index:   \", len(word_to_index), len(tag_to_index))\n",
    "print(\"Length      Dev Word / Tag Index:   \", len(word_to_index_dev), len(tag_to_index_dev))\n",
    "\n",
    "print(\"Word to index:     \", list(word_to_index.items())[-4:])\n",
    "print(\"Index to word:     \", list(index_to_word.items())[-4:])\n",
    "print(\"Word to index Dev: \", list(word_to_index_dev.items())[-4:])\n",
    "print(\"Index to word Dev: \", list(index_to_word_dev.items())[-4:])\n",
    "\n",
    "for (a, b) in zip(tag_to_index.items(), index_to_tag.items()):\n",
    "    print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sätze zu Integer übersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "['Er' 'waere' 'vielleicht' 'ein' 'praechtiger' 'Diktator' '-']\n",
      "['Er' 'waere' 'vielleicht' 'ein' 'praechtiger' 'Diktator' '-']\n",
      "[184   5   6   7   8   9 105]\n",
      "['PRON' 'VERB' 'ADV' 'DET' 'ADJ' 'NOUN' '.']\n",
      "['PRON' 'VERB' 'ADV' 'DET' 'ADJ' 'NOUN' '.']\n",
      "[9 3 4 5 6 2 1]\n",
      "\n",
      "Test Set. Might contain <UNK> words, because translate-index was created only with train set.\n",
      "['Mit' 'den' 'Einnahmen' 'will' 'Ecclestone' 'vor' 'allem']\n",
      "['Mit' 'den' 'Einnahmen' 'will' '<UNK>' 'vor' 'allem']\n",
      "[  257    13 10094   378     1   322   323]\n",
      "['ADP' 'DET' 'NOUN' 'VERB' 'NOUN' 'ADP' 'PRON']\n",
      "['ADP' 'DET' 'NOUN' 'VERB' 'NOUN' 'ADP' 'PRON']\n",
      "[7 5 2 3 2 7 9]\n",
      "\n",
      "Train Set Partial\n",
      "['Er' 'schloss' 'eine' 'Explosion' 'und' 'Sabotage' 'als']\n",
      "['Er' 'schloss' 'eine' 'Explosion' 'und' 'Sabotage' 'als']\n",
      "[191 192  49 193  40 194 186]\n",
      "['PRON' 'VERB' 'DET' 'NOUN' 'CONJ' 'NOUN' 'ADP']\n",
      "['PRON' 'VERB' 'DET' 'NOUN' 'CONJ' 'NOUN' 'ADP']\n",
      "[ 5  6  1  7 10  7  4]\n",
      "\n",
      "Dev Set. Might contain <UNK> words, because translate-index was created only with train set partial.\n",
      "['Aber' 'nur' 'die' 'Koelner' 'Philharmonie' 'lud' 'unlaengst']\n",
      "['Aber' 'nur' 'die' 'Koelner' '<UNK>' 'lud' 'unlaengst']\n",
      "[ 1163    99    42  3080     1 22260 25481]\n",
      "['CONJ' 'ADV' 'DET' 'ADJ' 'NOUN' 'VERB' 'ADV']\n",
      "['CONJ' 'ADV' 'DET' 'ADJ' 'NOUN' 'VERB' 'ADV']\n",
      "[10  2  1  3  7  6  2]\n"
     ]
    }
   ],
   "source": [
    "train_sents_words_int         = [translate(s, word_to_index,     word_to_index[\"<UNK>\"])     for s in train_sents_words]\n",
    "train_sents_tags_int          = [translate(s, tag_to_index,      tag_to_index[\"X\"])          for s in train_sents_tags]\n",
    "\n",
    "# the backup value \"X\" is never used because all tags are known\n",
    "test_sents_words_int          = [translate(s, word_to_index,     word_to_index[\"<UNK>\"])     for s in test_sents_words]\n",
    "test_sents_tags_int           = [translate(s, tag_to_index,      tag_to_index[\"X\"])          for s in test_sents_tags]\n",
    "\n",
    "train_sents_words_partial_int = [translate(s, word_to_index_dev, word_to_index_dev[\"<UNK>\"]) for s in train_sents_words_partial]\n",
    "train_sents_tags_partial_int  = [translate(s, tag_to_index_dev,  tag_to_index_dev[\"X\"])      for s in train_sents_tags_partial]\n",
    "\n",
    "dev_sents_words_int           = [translate(s, word_to_index_dev, word_to_index_dev[\"<UNK>\"]) for s in dev_sents_words]\n",
    "dev_sents_tags_int            = [translate(s, tag_to_index_dev,  tag_to_index_dev[\"X\"])      for s in dev_sents_tags]\n",
    "\n",
    "print(\"Train Set\")\n",
    "print(train_sents_words[15][:7])\n",
    "print(translate(train_sents_words_int[15],index_to_word,\"<UNK>\")[:7])\n",
    "print(train_sents_words_int[15][:7])\n",
    "\n",
    "print(train_sents_tags[15][:7])\n",
    "print(translate(train_sents_tags_int[15][:7],index_to_tag,\"X\"))\n",
    "print(train_sents_tags_int[15][:7])\n",
    "\n",
    "print()\n",
    "print(\"Test Set. Might contain <UNK> words, because translate-index was created only with train set.\")\n",
    "print(test_sents_words[15][:7])\n",
    "print(translate(test_sents_words_int[15],index_to_word,\"<UNK>\")[:7])\n",
    "print(test_sents_words_int[15][:7])\n",
    "\n",
    "print(test_sents_tags[15][:7])\n",
    "print(translate(test_sents_tags_int[15][:7],index_to_tag,\"X\"))\n",
    "print(test_sents_tags_int[15][:7])\n",
    "\n",
    "print()\n",
    "print(\"Train Set Partial\")\n",
    "print(train_sents_words_partial[15][:7])\n",
    "print(translate(train_sents_words_partial_int[15],index_to_word_dev,\"<UNK>\")[:7])\n",
    "print(train_sents_words_partial_int[15][:7])\n",
    "\n",
    "print(train_sents_tags_partial[15][:7])\n",
    "print(translate(train_sents_tags_partial_int[15][:7],index_to_tag_dev,\"X\"))\n",
    "print(train_sents_tags_partial_int[15][:7])\n",
    "\n",
    "print()\n",
    "print(\"Dev Set. Might contain <UNK> words, because translate-index was created only with train set partial.\")\n",
    "print(dev_sents_words[15][:7])\n",
    "print(translate(dev_sents_words_int[15],index_to_word_dev,\"<UNK>\")[:7])\n",
    "print(dev_sents_words_int[15][:7])\n",
    "\n",
    "print(dev_sents_tags[15][:7])\n",
    "print(translate(dev_sents_tags_int[15][:7],index_to_tag_dev,\"X\"))\n",
    "print(dev_sents_tags_int[15][:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sätze padden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184   5   6   7   8   9 105  74  30 185  92 186 187  36  10   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "['Er' 'waere' 'vielleicht' 'ein' 'praechtiger' 'Diktator' '-' 'aber' 'das'\n",
      " 'ist' 'nicht' 'unser' 'System' '.' \"''\" '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>']\n",
      "[ 9  3  4  5  6  2  1 11  9  3  8  9  2  1  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "['PRON' 'VERB' 'ADV' 'DET' 'ADJ' 'NOUN' '.' 'CONJ' 'PRON' 'VERB' 'PRT'\n",
      " 'PRON' 'NOUN' '.' '.' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>']\n",
      "[  257    13 10094   378     1   322   323    33    13    90 48603   564\n",
      "    65  1104    76     1   782   369    92    30  4403    50     1 22770\n",
      "    36     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "['Mit' 'den' 'Einnahmen' 'will' '<UNK>' 'vor' 'allem' 'in' 'den' 'USA'\n",
      " 'aktiver' 'werden' ',' 'wo' 'die' '<UNK>' 'bisher' 'noch' 'nicht' 'das'\n",
      " 'Geschaeft' 'mit' '<UNK>' 'dominieren' '.' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>']\n",
      "[7 5 2 3 2 7 9 7 5 2 6 3 1 9 5 2 4 4 8 5 2 7 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "['ADP' 'DET' 'NOUN' 'VERB' 'NOUN' 'ADP' 'PRON' 'ADP' 'DET' 'NOUN' 'ADJ'\n",
      " 'VERB' '.' 'PRON' 'DET' 'NOUN' 'ADV' 'ADV' 'PRT' 'DET' 'NOUN' 'ADP'\n",
      " 'NOUN' 'VERB' '.' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "[191 192  49 193  40 194 186 195  20  21 196 197 198 199 154  23   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0]\n",
      "['Er' 'schloss' 'eine' 'Explosion' 'und' 'Sabotage' 'als' 'Ursachen'\n",
      " 'fuer' 'das' 'Metro-Unglueck' 'vom' '28.' 'Oktober' 'aus' '.' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "[ 5  6  1  7 10  7  4  7  4  1  7  4  3  7 11  8  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "['PRON' 'VERB' 'DET' 'NOUN' 'CONJ' 'NOUN' 'ADP' 'NOUN' 'ADP' 'DET' 'NOUN'\n",
      " 'ADP' 'ADJ' 'NOUN' 'PRT' '.' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>']\n",
      "[ 1163    99    42  3080     1 22260 25481    42 25890    30    40 67896\n",
      "   294    10  1278  7310 36885   691     1 54047    23     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n",
      "['Aber' 'nur' 'die' 'Koelner' '<UNK>' 'lud' 'unlaengst' 'die' 'Budapester'\n",
      " 'ein' 'und' 'riskierte' 'es' ',' 'ihrem' 'Publikum' 'nacheinander' 'drei'\n",
      " '<UNK>' 'zuzumuten' '.' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n",
      "[10  2  1  3  7  6  2  1  7 11 10  6  5  8  5  7  2  9  7  6  8  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0]\n",
      "['CONJ' 'ADV' 'DET' 'ADJ' 'NOUN' 'VERB' 'ADV' 'DET' 'NOUN' 'PRT' 'CONJ'\n",
      " 'VERB' 'PRON' '.' 'PRON' 'NOUN' 'ADV' 'NUM' 'NOUN' 'VERB' '.' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>'\n",
      " '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>' '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "train_sents_words_int = keras.preprocessing.sequence.pad_sequences(train_sents_words_int, value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "train_sents_tags_int  = keras.preprocessing.sequence.pad_sequences(train_sents_tags_int,  value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "test_sents_words_int  = keras.preprocessing.sequence.pad_sequences(test_sents_words_int,  value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "test_sents_tags_int   = keras.preprocessing.sequence.pad_sequences(test_sents_tags_int,   value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "\n",
    "train_sents_words_partial_int = keras.preprocessing.sequence.pad_sequences(train_sents_words_partial_int, value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "train_sents_tags_partial_int  = keras.preprocessing.sequence.pad_sequences(train_sents_tags_partial_int,  value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "dev_sents_words_int           = keras.preprocessing.sequence.pad_sequences(dev_sents_words_int,           value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "dev_sents_tags_int            = keras.preprocessing.sequence.pad_sequences(dev_sents_tags_int,            value=word_to_index[\"<PAD>\"], padding='post',maxlen=train_max_sent_length)\n",
    "\n",
    "\n",
    "print(train_sents_words_int[15])\n",
    "print(translate(train_sents_words_int[15],index_to_word,index_to_word[1]))\n",
    "print(train_sents_tags_int[15])\n",
    "print(translate(train_sents_tags_int[15],index_to_tag,index_to_tag[1]))\n",
    "\n",
    "print(test_sents_words_int[15])\n",
    "print(translate(test_sents_words_int[15],index_to_word,index_to_word[1]))\n",
    "print(test_sents_tags_int[15])\n",
    "print(translate(test_sents_tags_int[15],index_to_tag,index_to_tag[1]))\n",
    "\n",
    "print(train_sents_words_partial_int[15])\n",
    "print(translate(train_sents_words_partial_int[15],index_to_word_dev,index_to_word_dev[1]))\n",
    "print(train_sents_tags_partial_int[15])\n",
    "print(translate(train_sents_tags_partial_int[15],index_to_tag_dev,index_to_tag_dev[1]))\n",
    "\n",
    "print(dev_sents_words_int[15])\n",
    "print(translate(dev_sents_words_int[15],index_to_word_dev,index_to_word_dev[1]))\n",
    "print(dev_sents_tags_int[15])\n",
    "print(translate(dev_sents_tags_int[15],index_to_tag_dev,index_to_tag_dev[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, SimpleRNN, GRU, Bidirectional, TimeDistributed, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_model(n_words_longest_sentence, n_distinct_words, n_distinct_tags):\n",
    "    \n",
    "    # https://keras.io/getting-started/sequential-model-guide/\n",
    "    model = Sequential()\n",
    "    # https://keras.io/layers/embeddings/\n",
    "    # Embedding(n_distinct_words + 1   :: (input_dim should equal size of vocabulary + 1)\n",
    "    # mask_zero = True                 :: ignores padding\n",
    "    model.add(Embedding(n_distinct_words + 1, 128, mask_zero=True, input_shape=(n_words_longest_sentence, )))\n",
    "    # https://keras.io/layers/recurrent/\n",
    "    # https://nlpforhackers.io/lstm-pos-tagger-keras/\n",
    "    # https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "    # return_sequences=True            :: needed for TimeDistributed\n",
    "    # output_shape                     :: 128 gives best results    \n",
    "    # go_backwards=True leads to bad performance\n",
    "    # SimpleRNN  97.1 - 97.2   \n",
    "    # GRU        97.2 - 97.7   \n",
    "    # LSTM       97.4 - 97.8   best with small batch size (2)\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    # https://keras.io/layers/core/\n",
    "    model.add(TimeDistributed(Dense(n_distinct_tags)))   \n",
    "    # https://www.dlology.com/blog/how-to-choose-last-layer-activation-and-loss-function/\n",
    "    # Multi-class, single-label classification :: Activation(softmax) loss='categorical_crossentropy'\n",
    "    model.add(Dense(n_distinct_tags, activation='softmax'))\n",
    "    model.compile(#https://keras.io/losses/\n",
    "                  loss='categorical_crossentropy',\n",
    "                  # https://keras.io/optimizers/\n",
    "                  # https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
    "                  optimizer=Adam(0.001), # Adam performs better then 'rmsprop'\n",
    "                  # https://keras.io/metrics/\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(m, sents_words_int, sents_tags_int, validation_split):\n",
    "    hist = m.fit(sents_words_int, \n",
    "                    keras.utils.to_categorical(sents_tags_int), \n",
    "                    # because the batch_size is very small, best results are achieved already after 2 epochs\n",
    "                    # but each epoch takes longer, because more steps are made\n",
    "                    epochs=2,\n",
    "                    # https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
    "                    # https://stackoverflow.com/questions/35050753/how-big-should-batch-size-and-number-of-epochs-be-when-fitting-a-model-in-keras\n",
    "                    #                 best val_acc (best epoch)      ~training_time per epoch (4 core 2.4GHz)\n",
    "                    # batch_size=256  0.9690       (5 epochs)        171s \n",
    "                    # batch_size=128  0.9682       (5 epochs)        213s \n",
    "                    # batch_size=64   0.9701       (4 epochs)        269s\n",
    "                    # batch_size=16   0.9735       (3 epochs)        672s\n",
    "                    # batch_size=4    0.9758       (3 epochs)        2000s    \n",
    "                    # batch_size=2    0.9771       (2 epochs)        3400s  Best\n",
    "                    # batch_size=1    0.9741       (2 epochs)        8355s \n",
    "                    batch_size=2, # small batch_size leads to smaller loss, params are adjusted after each batch\n",
    "                    validation_split=validation_split,\n",
    "                    verbose=1)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilfsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(trained_model, sentences_to_predict, sentences_to_predict_int, true_tags, dictionnary, log_sent=None):\n",
    "    \n",
    "    predictions_categorical     = trained_model.predict(sentences_to_predict_int)\n",
    "    predictions_with_padding    = to_categorical_reverse(predictions_categorical)\n",
    "    predictions_without_padding = remove_padding(predictions_with_padding, true_tags)\n",
    "    predictions_tags            = [translate(tag, dictionnary,\"error\") for tag in predictions_without_padding]\n",
    "    \n",
    "    if log_sent:\n",
    "        print(\"Predictions Categorical \", predictions_categorical[log_sent])\n",
    "        print(\"Predictions with Padding\", predictions_with_padding[log_sent])\n",
    "        print(\"Predictions no   Padding\", predictions_without_padding[log_sent])\n",
    "        print(\"Orig words: \",     len(sentences_to_predict[log_sent]),\n",
    "              \"True tags: \",      len(true_tags[log_sent]),\n",
    "              \"Predicted tags: \", len(predictions_tags[log_sent]))\n",
    "        for (a, b, c) in zip(true_tags[log_sent], predictions_tags[log_sent], sentences_to_predict[log_sent]):\n",
    "            print(a,b,c)\n",
    "\n",
    "    return predictions_tags\n",
    "\n",
    "def calculate_custom_accuracy_without_padding(predicted_tags, true_tags):\n",
    "    # calculates accuracy withot padding\n",
    "    true_tags_flat = [tag for sentence in true_tags for tag in sentence]\n",
    "    predicted_tags_flat = [tag for sentence in predicted_tags for tag in sentence]\n",
    "\n",
    "    return accuracy_score(true_tags_flat, predicted_tags_flat)\n",
    "    \n",
    "    \n",
    "def run_model(_train_max_sent_length, \n",
    "              _index_to_word, \n",
    "              _index_to_tag, \n",
    "              \n",
    "              _train_sents_words_int, \n",
    "              _train_sents_tags_int, \n",
    "              \n",
    "              _test_sents_words, \n",
    "              _test_sents_words_int, \n",
    "              _test_sents_tags, \n",
    "              _test_sents_tags_int, \n",
    "              _validation_split):\n",
    "\n",
    "    model = create_model(_train_max_sent_length, len(_index_to_word), len(_index_to_tag))\n",
    "    model.summary()\n",
    "    history = fit_model(model, _train_sents_words_int, _train_sents_tags_int, _validation_split)\n",
    "    \n",
    "    # only two epochs used at the end, no fun in plotting those\n",
    "    # plot_history(history)\n",
    "    \n",
    "    sents_predicted_tags = predict_tags(model, \n",
    "                                        _test_sents_words, \n",
    "                                        _test_sents_words_int, \n",
    "                                        _test_sents_tags,\n",
    "                                        _index_to_tag,\n",
    "                                        15)\n",
    "\n",
    "    acc_keras  = model.evaluate(_test_sents_words_int, keras.utils.to_categorical(_test_sents_tags_int, len(_index_to_tag)))\n",
    "    print(\"Accuracy Keras: \", acc_keras[1] * 100)\n",
    "\n",
    "    acc_custom = calculate_custom_accuracy_without_padding(sents_predicted_tags, _test_sents_tags)\n",
    "    print(\"Accuracy Custom (manually ignoring padding): \", acc_custom * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell trainieren mit 90% der Trainingsdaten (davon 10% zur Validierung) und mit 10% der Trainingsdaten testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 130, 128)          9201536   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 130, 256)          263168    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 130, 13)           3341      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 130, 13)           182       \n",
      "=================================================================\n",
      "Total params: 9,468,227\n",
      "Trainable params: 9,468,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32400 samples, validate on 3600 samples\n",
      "Epoch 1/2\n",
      "32400/32400 [==============================] - 3688s 114ms/step - loss: 0.1681 - acc: 0.9461 - val_loss: 0.0828 - val_acc: 0.9739\n",
      "Epoch 2/2\n",
      "32400/32400 [==============================] - 3909s 121ms/step - loss: 0.0330 - acc: 0.9905 - val_loss: 0.0810 - val_acc: 0.9757\n",
      "Predictions Categorical  [[6.7208314e-09 4.6107753e-09 1.1907963e-03 ... 9.9837840e-01\n",
      "  7.2746104e-08 1.0743126e-06]\n",
      " [7.6444134e-14 3.3112693e-07 9.9939835e-01 ... 2.2150701e-04\n",
      "  4.5508210e-07 3.1092173e-07]\n",
      " [5.0625421e-10 9.9999428e-01 4.6490406e-08 ... 2.3031688e-11\n",
      "  9.1180263e-10 1.2420453e-06]\n",
      " ...\n",
      " [1.5796344e-06 2.1422775e-06 1.2083395e-08 ... 6.5889505e-10\n",
      "  8.1453322e-07 1.4031744e-05]\n",
      " [1.5796344e-06 2.1422775e-06 1.2083395e-08 ... 6.5889505e-10\n",
      "  8.1453322e-07 1.4031744e-05]\n",
      " [1.5796344e-06 2.1422775e-06 1.2083395e-08 ... 6.5889505e-10\n",
      "  8.1453322e-07 1.4031744e-05]]\n",
      "Predictions with Padding [10  2  1  3  7  6  2  1  3  1 10  6  5  8  5  7  2  9  7  6  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8]\n",
      "Predictions no   Padding [10  2  1  3  7  6  2  1  3  1 10  6  5  8  5  7  2  9  7  6  8]\n",
      "Orig words:  21 True tags:  21 Predicted tags:  21\n",
      "CONJ CONJ Aber\n",
      "ADV ADV nur\n",
      "DET DET die\n",
      "ADJ ADJ Koelner\n",
      "NOUN NOUN Philharmonie\n",
      "VERB VERB lud\n",
      "ADV ADV unlaengst\n",
      "DET DET die\n",
      "NOUN ADJ Budapester\n",
      "PRT DET ein\n",
      "CONJ CONJ und\n",
      "VERB VERB riskierte\n",
      "PRON PRON es\n",
      ". . ,\n",
      "PRON PRON ihrem\n",
      "NOUN NOUN Publikum\n",
      "ADV ADV nacheinander\n",
      "NUM NUM drei\n",
      "NOUN NOUN Bartok-Abende\n",
      "VERB VERB zuzumuten\n",
      ". . .\n",
      "4000/4000 [==============================] - 12s 3ms/step\n",
      "Accuracy Keras:  97.57137365341187\n",
      "Accuracy Custom (manually ignoring padding):  97.57299699566656\n"
     ]
    }
   ],
   "source": [
    "# Training Time ~2 hours using CPU\n",
    "run_model(train_max_sent_length, \n",
    "              index_to_word_dev, \n",
    "              index_to_tag_dev,\n",
    "          \n",
    "              train_sents_words_partial_int, \n",
    "              train_sents_tags_partial_int,\n",
    "          \n",
    "              dev_sents_words,\n",
    "              dev_sents_words_int,\n",
    "              dev_sents_tags,\n",
    "              dev_sents_tags_int,\n",
    "              0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell trainieren mit 100% der Trainingsdaten und mit POS_German_minitest.txt testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 130, 128)          9812352   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 130, 256)          263168    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 130, 13)           3341      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 130, 13)           182       \n",
      "=================================================================\n",
      "Total params: 10,079,043\n",
      "Trainable params: 10,079,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 4970s 124ms/step - loss: 0.1494 - acc: 0.9531\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 4820s 121ms/step - loss: 0.0291 - acc: 0.9915\n",
      "Predictions Categorical  [[9.0430768e-10 2.5329872e-10 1.7422338e-06 ... 2.0075470e-06\n",
      "  3.6228448e-05 8.4130125e-10]\n",
      " [7.0927370e-10 1.9370254e-09 1.1134790e-07 ... 1.2060173e-07\n",
      "  6.7904354e-10 2.1664703e-06]\n",
      " [1.9098657e-17 1.3546241e-23 9.9999642e-01 ... 3.2175069e-06\n",
      "  1.3571506e-12 3.6918668e-10]\n",
      " ...\n",
      " [2.6623565e-07 9.9999475e-01 4.7397919e-10 ... 2.7872691e-09\n",
      "  1.8265793e-06 4.7679529e-07]\n",
      " [2.6623565e-07 9.9999475e-01 4.7397919e-10 ... 2.7872691e-09\n",
      "  1.8265793e-06 4.7679529e-07]\n",
      " [2.6623565e-07 9.9999475e-01 4.7397919e-10 ... 2.7872691e-09\n",
      "  1.8265793e-06 4.7679529e-07]]\n",
      "Predictions with Padding [7 5 2 3 2 7 9 7 5 2 6 3 1 9 5 2 4 4 8 5 2 7 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predictions no   Padding [7 5 2 3 2 7 9 7 5 2 6 3 1 9 5 2 4 4 8 5 2 7 2 3 1]\n",
      "Orig words:  25 True tags:  25 Predicted tags:  25\n",
      "ADP ADP Mit\n",
      "DET DET den\n",
      "NOUN NOUN Einnahmen\n",
      "VERB VERB will\n",
      "NOUN NOUN Ecclestone\n",
      "ADP ADP vor\n",
      "PRON PRON allem\n",
      "ADP ADP in\n",
      "DET DET den\n",
      "NOUN NOUN USA\n",
      "ADJ ADJ aktiver\n",
      "VERB VERB werden\n",
      ". . ,\n",
      "PRON PRON wo\n",
      "DET DET die\n",
      "NOUN NOUN Formel-1-Veranstaltungen\n",
      "ADV ADV bisher\n",
      "ADV ADV noch\n",
      "PRT PRT nicht\n",
      "DET DET das\n",
      "NOUN NOUN Geschaeft\n",
      "ADP ADP mit\n",
      "NOUN NOUN Autorennen\n",
      "VERB VERB dominieren\n",
      ". . .\n",
      "473/473 [==============================] - 2s 4ms/step\n",
      "Accuracy Keras:  97.4832314769511\n",
      "Accuracy Custom (manually ignoring padding):  97.38298362923679\n"
     ]
    }
   ],
   "source": [
    "run_model(train_max_sent_length, \n",
    "              index_to_word, \n",
    "              index_to_tag,\n",
    "          \n",
    "              train_sents_words_int, \n",
    "              train_sents_tags_int,\n",
    "          \n",
    "              test_sents_words,\n",
    "              test_sents_words_int,\n",
    "              test_sents_tags,\n",
    "              test_sents_tags_int,\n",
    "              0.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}